## 2.1 Model Identification ≠ Perspectival Information

Learning the device's rule is genuine information; after it's known, the per-flip fact of "we disagree" carries no further surprise—it is exactly what the rule predicts. Before you discover the rule, several live hypotheses compete (e.g., "always-same," "always-opposite," "independent"). Observing outcomes drains that model uncertainty. That is not the irreducible information we speak on here.

Formally, if $M$ denotes which rule is true and $D_{1:k}$ the first $k$ observations, the information gained about the rule is the drop in uncertainty (Cover & Thomas, 2006):

$$
I(M; D_{1:k}) \;=\; H(M)\;-\;H\!\left(M\mid D_{1:k}\right).
$$
With a uniform prior over the three hypotheses, two consecutive "opposite" outcomes yield the posterior $(0.8,\,0.2,\,0)$ (in the order "always-opposite," "independent," "always-same"), cutting entropy from $\log_2 3 \approx 1.585$ bits to about $0.722$ bits.

You are learning—but thereafter each new row shaves off less and less. **Intuitively: the surprise lives in discovering the rule**.  Once your posterior has essentially collapsed, "we disagree—again" is confirmation, not news. Each flip still tells you which joint outcome happened—that's one bit about the event—but it no longer tells you anything fresh about the governing rule. So the first Lenticular Coin sits at the *model-identification layer*: you infer the rule that governs the observations.

That is standard Shannon/Bayesian territory—useful, but not yet our target notion. It shows that perspective changes what you see, not what is true: there is a single global rule, simply viewed from different seats.

Once viewpoint is modeled within the state, **one law explains everything**.

1. $\text{LEFT}$ and $\text{RIGHT}$ always disagree;
2. $\text{HEADS}$ → $\text{LEFT}$ says $\text{YES}$ ($\text{RIGHT}$ says $\text{NO}$),
3. $\text{TAILS}$ → $\text{RIGHT}$ says $\text{YES}$ ($\text{LEFT}$ says $\text{NO}$).

The "law" is more than a lookup table here; it is the rule everyone follows when turning what they see into a report. In information-theoretic terms, it is the channel $p(o\mid s,p)$; in plain terms, it is the shared reporting language that makes my "$\text{YES}$" mean the same thing as your "$\text{YES}$". This matters because, once the law is fixed, **records should cohere**: different seats can yield different entries, but all entries are expected to fit under the same rule. We will use this distinction shortly.

To continue, we'll use a mundane feature of lenticular media: the transition band. It introduces a lawful "both" outcome—legitimate ambiguity—where "what happened" begins to blur. This is where a frame-independent summary begins to fail unless the context label is carried along; the reports remain consistent, but the summary without frames does not.

This pressure toward contradiction will become explicit in §2.3.