## 6.2 Lossless Compression with Context Knowledge

**Theorem 7** *(Optimal Compression, Known Contexts)*

With $C^n$ available to the decoder:

$$
\lim_{n \to \infty} \frac{1}{n} \mathbb{E}[\ell_n^*] = H(X|C) + K(P)
$$

with a strong converse. (see App. A.9)

**Theorem 8** *(Compression with Latent Contexts)*

Without access to $C^n$ at encoder or decoder:

$$
\lim_{n \to \infty} \frac{1}{n} \mathbb{E}[\ell_n^*] = H(X) + K(P)
$$

with a strong converse. (see App. A.9)

**Proof Strategy for Both:**

The converses follow from Theorem 9: any compression rate below these thresholds would require simulating $P$ with a witness rate $<K(P)$, which would imply a hypothesis test exceeding the type-II exponent bound in Theorem 9 â€” impossible.