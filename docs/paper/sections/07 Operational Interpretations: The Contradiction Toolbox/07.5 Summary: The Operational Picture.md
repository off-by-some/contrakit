## 7.5 Summary: The Operational Picture

The theorems in Sections 6-7 establish that $K(P) = -\log_2 \alpha^\star(P)$ is a universal **information tax** in every multi-context problem:

- **Storage:**
  Typical sets expand by factor $2^{nK(P)}$ (Theorem 6, Appendix A.3.2, Appendix A.2.2, Appendix A.5.1, Appendix A.9)
- **Compression:**
  Rates increase by exactly $K(P)$ bits/symbol (Theorems 7-8, Appendix A.9, Appendix A.5.1)
- **Communication:**
  Channel capacity decreases by $K(P)$ (Theorem 13, Appendix A.10, Appendix A.9)
- **Lossy Coding:**
  Rate-distortion functions shift up by $K(P)$ (Theorem 14, Appendix A.10, Appendix A.9)
- **Testing:**
  Detection requires $K(P)$ extra bits of evidence (Theorem 9, Appendix A.3.2, Appendix A.9)
- **Simulation:**
  Variance costs grow exponentially in $K(P)$ (Proposition 7.2, Appendix A.2.2)

The **witness mechanism** provides the unifying explanation: contexts must coordinate through short certificates of rate $K(P)$ to maintain consistency. When witnesses are underfunded, some decoder must fail—creating the fundamental tradeoff captured in Theorem 7.4.

The **geometric structure** (Theorem 15, Appendix A.2.2, Appendix A.10; FI product closure Appendix A.1.8) reveals that these costs arise from Hellinger angles in probability space. Frame-independence sits at the origin of a natural metric structure, with contradiction measured by worst-context distances that compose additively under products.

Together, these results show that **contradiction is not free**—it imposes an exact, universal tax on all information-theoretic operations, with the tax rate determined by the minimax game $\alpha^\star(P)$ between behaviors and frame-independent approximations.