## 9.2 Key Limitations

**Model Dependence of FI**:
The contradiction bit is only as meaningful as the chosen frame-independent set and context partition. $K(P)$ is invariant to outcome relabelings but not to FI specification or context grouping choices. We recommend sensitivity analysis—reporting $K(P)$ under alternative FI definitions and context partitions—and careful documentation of modeling assumptions. Robustness to FI misspecification is an open problem.

**Context Granularity Effects**:
Refining or merging contexts can change $K(P)$ values. While our grouping axiom prevents double-counting identical contexts, the partition structure itself affects the measure. Context design requires domain expertise and affects results.

**Computational Complexity**:
Frame-independent polytopes typically have exponentially many extremal points. Computing $\alpha^\star(P)$ exactly requires column generation or cutting plane methods for tractable implementation.

**Data Requirements**:
Estimating $K(P)$ requires sufficient samples in each context to estimate joint distributions. Small-sample bias, appropriate bootstrap confidence intervals for plug-in estimators $\hat{K}$, and concentration bounds remain open beyond toy settings.

**Statistical Estimation Barrier for Zero Probabilities**:
When true distributions $P$ contain exact zero probabilities, statistical estimation of $K(P)$ faces fundamental limitations. The plug-in estimator using empirical frequencies is statistically inconsistent: no finite sample size guarantees convergence to the true $K(P)$ value. This occurs because empirical distributions cannot distinguish between "zero probability" and "very small probability not yet observed." In practice, domain knowledge about impossible outcomes (as in the lenticular coin) enables accurate estimation, but general statistical methods require careful treatment of zeros.

**Statistical Consistency via Regularization**:
This barrier can be overcome using regularized estimators that add small pseudocounts $\epsilon > 0$ to all outcome counts before normalization. The regularized empirical distributions converge to the true distributions as sample size increases, and since $K(P)$ is continuous in $P$ (Axiom A2), the resulting contradiction estimates achieve $\sqrt{n}$-consistency. This enables reliable bootstrap confidence intervals and statistical inference for contradiction measures in practical applications.

**Noise Interaction**:
While $K(P)$ targets structural disagreement, Section 7.7 shows that adding frame-independent "noise" contracts $K$ predictably via
$$
K((1-t)P + tR) \leq -\log_2((1-t)2^{-K(P)} + t)
$$
This provides a smoothing mechanism but requires careful calibration.

**High Stability of Contradiction Measures**:
Contradiction measures exhibit extreme stability compared to entropy: reducing $K(P)$ from any positive value to near-zero requires mixing in nearly 100% frame-independent behavior. For example, the lenticular coin ($K \approx 0.29$ bits) requires $t \geq 99.6\%$ FI mixture to achieve $K \leq 0.001$ bits. This "stickiness" property distinguishes contradiction from gradual entropy reduction, indicating that perspectival incompatibility is a discrete phenomenon requiring massive intervention to eliminate.